{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EL HARKAOUI Chaymae -- 01-03-2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h1 style=color:red>LangChain</h1></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:magenta>What is LangChain ?</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain** is a <font color=gray>-- Python - JavaScript -</font> **Framework** designed to help building **applications powered by LLMs** such as **OpenAI's GPT** - **Google's Gemini** - **Meta's LLaMA** - and other similar models. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain** allows developers to :<br>\n",
    "\n",
    "- **Integrate** LLMs with **external data** -- APIs - databases - documents ...\n",
    "- Implement **memory** in chat applications - so conversations are stateful\n",
    "- Use **chains of LLM calls** to enhance reasoning and problem-solving.\n",
    "- **Combine** different models - tools - agents for complex tasks.\n",
    "\n",
    "**LangChain** makes it easy to build **AI-powered applications** like chatbots - document summarizers - question-answering systems - autonomous agents ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain was created in **October 2022** by **Harrison Chase** as an open-source framework to simplify working with LLMs. Initially, it focused on providing **wrappers for LLMs and prompt templates**, but as AI adoption surged especially after ChatGPT‚Äôs launch - it rapidly evolved into a **full ecosystem**.<br> \n",
    "\n",
    "By early **2023** - LangChain introduced **memory - chains - agent capabilities** - enabling chatbots - AI assistants - automation workflows. Its integration with **vector databases** like FAISS and Pinecone and support for **multiple LLM providers** - OpenAI - Hugging Face - Cohere ... made it widely adopted for **retrieval-augmented generation RAG and enterprise AI applications**.<br> \n",
    "\n",
    "By **2024** - LangChain had become a **leading AI development framework**, allowing developers to build **scalable - intelligent AI-driven applications** with ease. Its future promises **more advanced AI agents - better on-premise model support - improved enterprise scalability** - solidifying its role in the **next generation of AI-powered software**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Images/langchian.png](Images/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:magenta>Key Concepts in LangChain</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is built on five main **pillars** :<br>\n",
    "\n",
    "**<font color=blue>LLM Wrappers</font>** --- Easily interact with models like OpenAI‚Äôs GPT - Claude - LLaMA ...<br>\n",
    "**<font color=blue>Prompt Management</font>** --- Helps with prompt engineering and formatting.<br>\n",
    "**<font color=blue>Memory</font>** --- Maintains context across multiple interactions.<br>\n",
    "**<font color=blue>Data Connectivity</font>** --- Retrieves and processes structured or unstructured data.<br>\n",
    "**<font color=blue>Agents & Chains</font>** --- Enables AI systems to make decisions and take actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:magenta>Core Modules in LangChain</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<font color=navy>1 - Language Model Wrappers</font>** <br>\n",
    "**<font color=navy>2 - Prompt Templates</font>**<br>\n",
    "**<font color=navy>3 - Chains</font>**<br>\n",
    "**<font color=navy>4 - Memory</font>** <br>\n",
    "**<font color=navy>5 - Agents</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>LLM Wrappers</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Wrappers** provide a **unified interface** for **interacting** with various **language models** - OpenAI GPT - Anthropic Claude - Cohere - Hugging Face models ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Why to use ?</h5>**\n",
    "\n",
    "- Allow seamless **switching** between different models\n",
    "- **Standardize** API calls and response handling\n",
    "- **Reduce boilerplate code** when working with multiple LLM providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example without LangChain ‚ùå</h5>**\n",
    "\n",
    "To call OpenAI‚Äôs GPT model directly - we have to write custom API calls.<br><br>\n",
    "\n",
    "**Problems**\n",
    "\n",
    "- Requires **manual** API handling\n",
    "- Switching between models requires **rewriting code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Our OpenAI API key\n",
    "openai.api_key = 'sk-projKuYvXPYyKUAbLyZd0oxyfxfpNqMCUA'\n",
    "\n",
    "# Call the Chat API \n",
    "response = openai.completions.create( model=\"gpt-3.5-turbo\", prompt=\"What is LangChain?\", max_tokens=100 )\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example with LangChain ‚úÖ</h5>**\n",
    "\n",
    "**Advantages** \n",
    "- Less boilerplate code\n",
    "- Easily interchangeable models\n",
    "- Integrated with other LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI with our API key\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"svrFJ0ix87AZiZ-CkMdYvXPYyKUAbLyZd0oxyfxfpNqMCUA\")\n",
    "\n",
    "# Prepare the message \n",
    "messages = [ {\"role\": \"user\", \"content\": \"What is LangChain?\"} ]\n",
    "\n",
    "# Invoke the model \n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>Prompt Templates</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Templates** allow us to dynamically **format** and manage **prompts** in a **structured way**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Why to use ?</h5>**\n",
    "\n",
    "- **Prevent** redundant prompt writing\n",
    "- Ensure **consistent structure** in requests\n",
    "- Allow **parameterized inputs** for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example without LangChain ‚ùå</h5>**\n",
    "\n",
    "**Problems**\n",
    "\n",
    "- Hardcoded prompts\n",
    "- Difficult to scale for multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Quantum Computing\"\n",
    "\n",
    "prompt = f\"Explain {user_input} in simple terms.\"\n",
    "\n",
    "response = openai.ChatCompletion.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}] )\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example with LangChain ‚úÖ</h5>**\n",
    "\n",
    "**Advantages** \n",
    "- Reusability for different inputs\n",
    "- Standardized prompt structures\n",
    "- Easy modifications and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Quantum Computing in simple terms.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate( input_variables=[\"topic\"], template=\"Explain {topic} in simple terms.\")\n",
    "\n",
    "formatted_prompt = template.format(topic=\"Quantum Computing\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>Chains</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain - a **chain** means a **sequence of steps** that process **input** and generate **output**.<br>\n",
    "\n",
    "A **basic** chain **links** a **prompt** to an **LLM**- It links a **prompt template** with an **LLM** to form a **structured pipeline** for **generating responses**.<br>\n",
    "\n",
    "A **complex** chain can **combine multiple steps** - like retrieving data - applying logic - using different models...<br>\n",
    "\n",
    "Think of it like a conveyor belt : **User input query** ‚Üí **Prompt Formatting** ‚Üí **LLM Processing** ‚Üí **Output Generation**<br>\n",
    "\n",
    "**Chains** allow you to **<font color=purple>connect multiple components</font>** - LLMs - memory - tools ... into a **<font color=purple>single workflow</font>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example without LangChain ‚ùå</h5>**\n",
    "\n",
    "If we want to take a user input - format a prompt - get an LLM response - we need to :<br><br>\n",
    "\n",
    "**Problems**\n",
    "\n",
    "- Each step - formatting ‚Üí sending ‚Üí retrieving - is manually coded\n",
    "- Hard to extend for complex workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Neural Networks\"\n",
    "\n",
    "prompt = f\"Explain {user_input} in simple terms.\"\n",
    "\n",
    "response = openai.ChatCompletion.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}] )\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example with LangChain ‚úÖ</h5>**\n",
    "\n",
    "**Advantages** \n",
    "-  Automates chaining of prompts and responses\n",
    "- Easily extendable with memory - tools and multiple steps\n",
    "- Cleaner - reusable code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", openai_api_key=\"our_api_key\")\n",
    "\n",
    "template = PromptTemplate( input_variables=[\"topic\"],template=\"Explain {topic} in simple terms.\" )\n",
    "\n",
    "# A chain that connects a Langauge Model with a structured prompt\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "\n",
    "response = chain.run(\"Neural Networks\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>Memory</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory** allows LLMs to remember **previous conversations** and maintain context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Why to use ?</h5>**\n",
    "\n",
    "- Enables stateful conversations\n",
    "- Avoids repetition in chatbot applications\n",
    "- Makes LLMs behave more like a human assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example without LangChain ‚ùå</h5>**\n",
    "\n",
    "Each message must contain context **manually**.<br><br>\n",
    "\n",
    "**Problems**\n",
    "\n",
    "- We must manually track the conversation history\n",
    "- Becomes inefficient for long interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list defines a structured conversation history for the chatbot\n",
    "# Each dictionary in the list represents a message in the conversation\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What was my first message?\"}\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create( model=\"gpt-4\", messages=messages )\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example with LangChain ‚úÖ</h5>**\n",
    "\n",
    "**Advantages** \n",
    "-  Automatic conversation tracking\n",
    "- Supports long-term memory\n",
    "- Scalable for chatbots and personal assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", openai_api_key=\"our_api_key\")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.run(\"Hello!\"))\n",
    "print(conversation.run(\"What was my first message?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>Agents</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents** allow an **LLM** to **interact** with **external tools** and dynamically decide which tool to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Why to use ?</h5>**\n",
    "\n",
    "- Enables AI-driven decision-making\n",
    "- Connects LLMs with APIs - databases - web scraping\n",
    "- Reduces the need for hardcoded responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example without LangChain ‚ùå</h5>**\n",
    "\n",
    "**Problems**\n",
    "\n",
    "- Manually coded logic for each tool\n",
    "- Hard to scale with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city):\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "query = \"What's the weather in Paris?\"\n",
    "if \"weather\" in query:\n",
    "    response = get_weather(\"Paris\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Example with LangChain ‚úÖ</h5>**\n",
    "\n",
    "**Advantages** \n",
    "- Dynamically selects the correct tool\n",
    "- Automates AI-driven decision-making\n",
    "- Scalable for multiple tools - web search - APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def get_weather(city):\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"WeatherAPI\",\n",
    "    func=get_weather,\n",
    "    description=\"Fetches weather data for a given city.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[weather_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent.run(\"What's the weather in Paris?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:olive>Indexes</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain - an **index** is a **structured way** to **store - organize - retrieve information** efficiently when working with **large datasets** or **document collections**. <br>\n",
    "\n",
    "Indexes are especially useful for retrieval-augmented generation RAG - where an LLM **fetches** relevant **information** before **generating a response**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Why to use ?</h5>**\n",
    "\n",
    "- Efficiently **search large datasets** instead of scanning everything\n",
    "- Improve response accuracy by **retrieving relevant documents** before calling an LLM\n",
    "- Handle **knowledge retrieval** for applications like chatbots - question answering - document search ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:navy>Types of Indexes in LangChain ?</h5>**\n",
    "\n",
    "- **Vector Index** - Embedding-Based\n",
    "- **Keyword Index** - Text-Based\n",
    "- **Structured Index** - SQL-Based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:teal>Vector Index</h5>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Converts **text** into **vector embeddings** and **stores** them in a **vector database** like FAISS - Pinecone - Weaviate - ChromaDB ...<br>\n",
    "It Uses **similarity search** to find the most relevant data points.<br>\n",
    "It is Ideal for semantic search - Q&A - knowledge retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create an embedding model\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"LangChain helps build LLM applications.\", \"Vector databases store embeddings for search.\"]\n",
    "\n",
    "# Convert documents into a FAISS vector index\n",
    "vector_index = FAISS.from_texts(documents, embedding_model)\n",
    "\n",
    "# Retrieve relevant information\n",
    "query = \"What does LangChain do?\"\n",
    "similar_docs = vector_index.similarity_search(query)\n",
    "\n",
    "print(similar_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:teal>Keyword Index</h5>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It **Stores documents** in a simple **text-based format**.<br>\n",
    "Uses **traditional keyword matching** for **search**.<br>\n",
    "Best for smaller datasets where full-text search is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load documents from a text file\n",
    "loader = TextLoader(\"documents.txt\")\n",
    "\n",
    "# Create a keyword-based index\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Query the index\n",
    "response = index.query(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:teal>Structured Index</h5>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Uses **structured databases** to store and retrieve information.<br>\n",
    "Useful for retrieving structured data like user profiles - transactions - logs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chains import SQLDatabaseChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Connect to an SQL database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///my_database.db\")\n",
    "\n",
    "# Create a query chain\n",
    "chain = SQLDatabaseChain(llm=OpenAI(), database=db)\n",
    "\n",
    "# Ask a question that requires structured retrieval\n",
    "response = chain.run(\"How many users signed up in January?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=color:olive></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:magenta>Example -- Chat Application with Streamlit</h3></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 12:48:56.367 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.147 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\MTechno\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-18 12:48:57.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\MTechno\\AppData\\Local\\Temp\\ipykernel_20140\\582130594.py:33: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  title_memory = ConversationBufferMemory(input_key='topic', memory_key='chat_history')  # Memory for title generation\n",
      "C:\\Users\\MTechno\\AppData\\Local\\Temp\\ipykernel_20140\\582130594.py:38: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.9)  # Higher temperature makes responses more creative\n",
      "C:\\Users\\MTechno\\AppData\\Local\\Temp\\ipykernel_20140\\582130594.py:42: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key='title', memory=title_memory)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary dependencies\n",
    "import os  # Used to set environment variables\n",
    "import streamlit as st  # Streamlit for creating the web app\n",
    "from langchain.llms import OpenAI  # OpenAI's language model integration\n",
    "from langchain.prompts import PromptTemplate  # For structuring prompts\n",
    "from langchain.chains import LLMChain, SequentialChain  # Chains for managing model interactions\n",
    "from langchain.memory import ConversationBufferMemory  # Memory to store conversation history\n",
    "from langchain.utilities import WikipediaAPIWrapper  # Wikipedia API for fetching relevant data\n",
    "\n",
    "# Set OpenAI API Key (ensure to keep this secret in production)\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "\n",
    "# Create the Streamlit app interface\n",
    "st.title('ü¶úüîó YouTube GPT Creator')  # App title\n",
    "prompt = st.text_input('Plug in your prompt here')  # User input field\n",
    "\n",
    "\n",
    "# Define Prompt Templates\n",
    "# Template for generating a YouTube video title based on a given topic\n",
    "title_template = PromptTemplate(\n",
    "    input_variables=['topic'],  # The expected input variable\n",
    "    template='write me a youtube video title about {topic}')\n",
    "\n",
    "\n",
    "# Template for generating a YouTube video script using Wikipedia research\n",
    "script_template = PromptTemplate(\n",
    "    input_variables=['title', 'wikipedia_research'],  # Uses title and research data\n",
    "    template='write me a youtube video script based on this title TITLE: {title} while leveraging this wikipedia research: {wikipedia_research}')\n",
    "\n",
    "\n",
    "# Define memory buffers to store conversation history\n",
    "title_memory = ConversationBufferMemory(input_key='topic', memory_key='chat_history')  # Memory for title generation\n",
    "script_memory = ConversationBufferMemory(input_key='title', memory_key='chat_history')  # Memory for script generation\n",
    "\n",
    "\n",
    "# Initialize OpenAI language model with a specific temperature setting\n",
    "llm = OpenAI(temperature=0.9)  # Higher temperature makes responses more creative\n",
    "\n",
    "\n",
    "# Create LLM chains for title and script generation\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key='title', memory=title_memory)\n",
    "script_chain = LLMChain(llm=llm, prompt=script_template, verbose=True, output_key='script', memory=script_memory)\n",
    "\n",
    "\n",
    "# Initialize Wikipedia API wrapper to fetch related content\n",
    "wiki = WikipediaAPIWrapper()\n",
    "\n",
    "\n",
    "# Check if the user has entered a prompt\n",
    "if prompt:\n",
    "    # Generate a video title using the input prompt\n",
    "    title = title_chain.run(prompt)\n",
    "\n",
    "    # Fetch related information from Wikipedia\n",
    "    wiki_research = wiki.run(prompt)\n",
    "\n",
    "    # Generate a video script using the title and Wikipedia research\n",
    "    script = script_chain.run(title=title, wikipedia_research=wiki_research)\n",
    "\n",
    "    # Display the generated title\n",
    "    st.write(title)\n",
    "\n",
    "    # Display the generated script\n",
    "    st.write(script)\n",
    "\n",
    "    # Expandable sections for viewing conversation history\n",
    "    with st.expander('Title History'):\n",
    "        st.info(title_memory.buffer)  # Show past generated titles\n",
    "\n",
    "    with st.expander('Script History'):\n",
    "        st.info(script_memory.buffer)  # Show past generated scripts\n",
    "\n",
    "    with st.expander('Wikipedia Research'):\n",
    "        st.info(wiki_research)  # Show Wikipedia research used for the script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
